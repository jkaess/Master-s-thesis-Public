{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0fcdec8c",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c549389a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import time\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from pathlib import Path\n",
    "from ollama import chat\n",
    "from pydantic import BaseModel\n",
    "from typing import Any\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197ba6c8",
   "metadata": {},
   "source": [
    "## 1. Document import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09a4905b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def documentImporter(file_path):\n",
    "    \"\"\"\n",
    "    Imports a text file and returns its content.\n",
    "    \"\"\"\n",
    "    if os.path.exists(file_path):\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            TextInhalt = file.read()\n",
    "        return TextInhalt\n",
    "    else:\n",
    "        print(f\"File not found: {file_path}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4e255d",
   "metadata": {},
   "source": [
    "Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530a0dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"/home/pc/Uni/MasterThesis/Speeches/IndividualSessions/documents_TextDaten1951-12-01-1952-01-01.txt_3_4599.txt\"\n",
    "match = re.search(r'(\\d{4})\\.txt$', filepath)\n",
    "documentId = match.group(1) if match else \"TextClean\"\n",
    "documentId = \"TextClean\"\n",
    "\n",
    "RawText = documentImporter(filepath, documentId)\n",
    "del filepath\n",
    "del documentId\n",
    "del match"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e89be0d7",
   "metadata": {},
   "source": [
    "## 2. Retrieving speechcontext\n",
    "### Speechdate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c833c4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dategetter(RawText):\n",
    "\n",
    "    datelist = re.findall(r'\"datum\":\"\\d{4}-\\d{2}-\\d{2}\"', RawText)\n",
    "    date_list = []\n",
    "    for n_date in datelist:\n",
    "        date_str = n_date.split('\"')[3]  # Extract the date string from the match\n",
    "        date_list.append(dt.datetime.strptime(date_str, '%Y-%m-%d').date())\n",
    "\n",
    "    # Only every even entry is relevant. This is the case because the ID is included in each document twice. Removing the uneven entries\n",
    "    date_list = date_list[::2]\n",
    "    \n",
    "    return date_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f1df04",
   "metadata": {},
   "source": [
    "Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135056ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "date_list = []\n",
    "date_list = dategetter(RawText)\n",
    "print(date_list)\n",
    "\n",
    "Documents = {\n",
    "    \"TextClean\": RawText,\n",
    "    \"Date\": date_list,\n",
    "}\n",
    "Documents_df = pd.DataFrame(Documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f26b0d2c",
   "metadata": {},
   "source": [
    "### Speaker name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8dcf2883",
   "metadata": {},
   "outputs": [],
   "source": [
    "def name_and_party_getter(Chunk, patterns):\n",
    "    for pattern in patterns:\n",
    "        matches = re.findall(pattern, Chunk)\n",
    "        if matches:\n",
    "            return matches[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2f03fa",
   "metadata": {},
   "source": [
    "## 3. Defining content patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91aeff68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store patterns in dictionaries for better organization\n",
    "patterns = {\n",
    "    \"preamble\": re.compile(r'(\"id\":\"\\d{4}\")(.*?)Uhr.{0,150}(Alterspräsidentin|Alterspräsident|Vizepräsidentin|Vizepräsident|Vizekanzlerin|Vizekanzler|Präsidentin|Präsident|Kanzlerin|Kanzler).{0,50}?:', re.DOTALL), #I swear this makes sense!\n",
    "    \"appendix\": re.compile(r'(\\(Schluß der Sitzung: \\d+(.|:)\\d+ Uhr.?\\)|\\\\nAnlagen zum Stenographischen Bericht|\\\\nAnlage 1)(.*?)(\"id\":\"\\d{4})', re.DOTALL),\n",
    "    \"appendix_last\": re.compile(r'(\\(Schluß der Sitzung: \\d+(.|:)\\d+ Uhr.?\\)|\\\\nAnlagen zum Stenographischen Bericht|\\\\nAnlage 1)(.*?)', re.DOTALL),    # Last appendix without \"id\" at the end\n",
    "    \"party_speaker\": re.compile(r'[^\\s,]+ [^\\s,]+ \\([^\\s,]+\\)\\s?:', re.DOTALL),                                                                         # Generic pattern for speeches, e.g. 'Speaker (Party) :'                                           \n",
    "    \"party_speaker_CDU\": re.compile(r'[^\\s,]+ [^\\s,]+ \\(CDU/CSU\\)\\s?:', re.DOTALL),                                                                     # Specific pattern for CDU speeches\n",
    "    \"party_speaker_FDP_random\": re.compile(r'[^\\s,]+ [^\\s,]+ \\(F.D.P.\\)\\s?:', re.DOTALL),                                                               # For an ungodly reason the FDP was briefly referred to as F.D.P. 1999-2000. I suspect this is a conspiracy to sabotage my thesis and social science in general.\n",
    "    \"party_speaker_new\": re.compile(r'\\[\\w+\\]\\s?', re.DOTALL),                                                                                          # New pattern for speeches after 2013. The previous pattern 'Speaker (Party) :' was replaced with 'Speaker [Party] :' in the Bundestag protocol.\n",
    "    \"party_speaker_CDU_new\": re.compile(r'\\[CDU+/CSU\\]\\s?:', re.DOTALL),                                                                                # Specific pattern for CDU speeches\n",
    "    \"minister_speaker\": re.compile(r'(?:[^\\n,]+,\\s+Bundesminister(?:in)?\\s+(?:der|für|des)\\s+[^\\n:]+:)', re.DOTALL | re.UNICODE),                      # Ministers are usually addressed with 'Bundesminister der ... i.e. Finanzen'\n",
    "    \"chancellor_speaker\": re.compile(r', (?:(?:Bundes|Vize)?[Kk]anzlerin?):', re.DOTALL),                                                                 # Chancellor speeches are usually addressed with 'Bundeskanzlerin:' or 'Vizekanzlerin:'\n",
    "    \"reactions\": re.compile(r'\\(\\w\\w+ (.*?)\\)', re.DOTALL),                                                                                             # Reactions are usually in the form '(Applaus)', '(Beifall)', '(Zuruf)', these simple reactions are removed here\n",
    "    \"remarks\": re.compile(r'\\((?!CDU/CSU|CDU|CSU|SPD|FDP|F.D.P.|AfD|Die Linke|Bündnis 90/Die Grünen|Bündnis 90 / Die Grüne|Die Grünen|LINKE|PDS|Piraten|NPD|REP|DVU|ÖDP|Tierschutzpartei|MLPD|DKP|BP|SSW|Fraktionslos)[^(]*?:[^()]+\\)', re.DOTALL) # Excludes party markers i.e. --> Joachim Gauck (CDU) : I need to keep these to identify individual speeches which\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d26e4b",
   "metadata": {},
   "source": [
    "## 4. Isolating session content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "93f1f4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def isolate_session_content(RawText):\n",
    "    '''The sequence of removing preamble (Table of content, list of appendices etc.) and appendix (Appendix, list of speakers etc.) is important.\n",
    "    If the appendix is removed first, the preamble will not be removed correctly, because it relies on the presence of the appendix to identify the end of the preamble.'''\n",
    "    # Remove preamble first\n",
    "    textIsolated = patterns[\"preamble\"].sub(r'\\1\\3', RawText)\n",
    "\n",
    "    # Remove appendix patterns\n",
    "    textIsolated = patterns[\"appendix\"].sub(r'\\4', textIsolated)\n",
    "    textIsolated = patterns[\"appendix_last\"].sub(\"\", textIsolated)\n",
    "\n",
    "    return textIsolated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7726ae91",
   "metadata": {},
   "outputs": [],
   "source": [
    "Isolated_text = isolate_session_content(RawText)\n",
    "del RawText"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98428904",
   "metadata": {},
   "source": [
    "### Isolate single sessions (Redundant see KeywordSearch.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a073ca03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_sessions_by_id(text):\n",
    "\n",
    "    documentIds = []\n",
    "    documentIds = re.findall(r'\"id\":\"\\d{4}\"', text)\n",
    "    documentIds = documentIds[::2]\n",
    "    print(documentIds)\n",
    "    \n",
    "    matches = list(re.finditer(r'\"id\":\"\\d{4}\"', text))\n",
    "    sessions = []\n",
    "    for i in range(len(matches)):\n",
    "        start = matches[i].start()\n",
    "        end = matches[i+1].start() if i+1 < len(matches) else len(text)\n",
    "        sessions.append(text[start:end])\n",
    "\n",
    "    for i, session in enumerate(session_texts):\n",
    "        documentID = re.search(r'\"id\":\"(\\d{4})\"', session)\n",
    "        session_file_name = f\"{documentID.group(1)}.txt\"\n",
    "        with open(session_file_name, \"w\", encoding=\"utf-8\") as file:\n",
    "            file.write(session)\n",
    "        print(f\"Session {i+1} saved as {session_file_name}\")\n",
    "\n",
    "    return sessions\n",
    "\n",
    "session_texts = split_sessions_by_id(isolate_session_content(RawText))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4dd94e",
   "metadata": {},
   "source": [
    "## 5. Chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3576b779",
   "metadata": {},
   "source": [
    "### Extracting all remarks and removing all reactions Step 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5686d852",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reactions_remarks_processing(text):\n",
    "    remarksList = re.findall(patterns[\"remarks\"], text)\n",
    "    text = re.sub(patterns[\"remarks\"], \"\", text)\n",
    "    text = re.sub(patterns[\"reactions\"], \"\", text)\n",
    "    return text, remarksList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39dd567e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    IsolatedText = isolate_session_content(RawText)\n",
    "    ProcessedText = reactions_remarks_processing(IsolatedText)\n",
    "    print(ProcessedText[0][:500])  # Print the first 500 characters of the cleaned text for verification\n",
    "\n",
    "    del IsolatedText"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e9d0ac",
   "metadata": {},
   "source": [
    "### Recursive character splitting using regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b0e5b180",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter_pre2013 = RecursiveCharacterTextSplitter(\n",
    "    \n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=100,\n",
    "    length_function=len,\n",
    "    is_separator_regex=True,\n",
    "\n",
    "    separators=[\n",
    "        patterns['party_speaker'].pattern,\n",
    "        patterns['party_speaker_CDU'].pattern,\n",
    "        patterns['chancellor_speaker'].pattern,\n",
    "        patterns['minister_speaker'].pattern,\n",
    "        patterns['party_speaker_FDP_random'].pattern,\n",
    "    ]\n",
    ")\n",
    "\n",
    "# The utilization of two splitter is necessary due to some structural changes in the protocols after 2013 as the patterns changed. Don't as me  why, I just work here.\n",
    "\n",
    "text_splitter_post2013 = RecursiveCharacterTextSplitter(\n",
    "\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=100,\n",
    "    length_function=len,\n",
    "    is_separator_regex=True,\n",
    "    keep_separator=True,\n",
    "    \n",
    "    separators=[\n",
    "        patterns['chancellor_speaker'].pattern,\n",
    "        patterns['minister_speaker'].pattern,\n",
    "        patterns['party_speaker_new'].pattern,\n",
    "        patterns['party_speaker_CDU_new'].pattern,\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8dbd99",
   "metadata": {},
   "source": [
    "### Chunk Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8e1ed79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_processing(chunks):\n",
    "    merged_chunks = []\n",
    "    i = 0\n",
    "    n = 0\n",
    "    while i < len(chunks):\n",
    "        current_chunk = chunks[i]\n",
    "\n",
    "        if len(current_chunk) >= 300 or n >= 3:\n",
    "            merged_chunks.append(current_chunk)\n",
    "            i += 1\n",
    "            n = 0\n",
    "\n",
    "        elif len(current_chunk) < 300 and i >= 1 and n < 4:\n",
    "            merged_chunks.append(chunks[i-1] + \" \" + current_chunk)\n",
    "            # The previous structure lead to looped failure as chunks were merged in a way that they were never longer than 300 characters.\n",
    "            n += 1\n",
    "            i += 1\n",
    "\n",
    "        else:\n",
    "            print(\n",
    "                f\"Chunk length is less than 300 characters: {len(current_chunk)}\")\n",
    "            print(\"Failure in merging chunks. Tiny chunk detected.\")\n",
    "            n += 1\n",
    "\n",
    "    return merged_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "24a80590",
   "metadata": {},
   "outputs": [],
   "source": [
    "def corpus_cleaner(Documents_df):\n",
    "    \n",
    "    pattern = re.compile(r'(.*?)(\\\\n)', re.DOTALL | re.MULTILINE)\n",
    "    for i in range(len(Documents_df['speaker'])):\n",
    "        if pd.notna(Documents_df.loc[i, 'speaker']) and Documents_df.loc[i, 'speaker'] is not None:\n",
    "            Documents_df.loc[i, 'speaker'] = pattern.sub(\n",
    "                '', Documents_df.loc[i, 'speaker'])\n",
    "\n",
    "    for i in range(len(Documents_df['chunk'])):\n",
    "        if pd.notna(Documents_df.loc[i, 'chunk']) and Documents_df.loc[i, 'chunk'] is not None:\n",
    "            Documents_df.loc[i, 'chunk'] = Documents_df.loc[i, 'chunk'].replace(\n",
    "                '\\\\n', ' ')\n",
    "\n",
    "    return Documents_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dee1853c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Year of session: 1974\n",
      "Number of chunks: 163\n",
      "processing chunks for /home/pc/Uni/MasterThesis/Speeches/IndividualSessions/documents_TextDaten1974-04-01-1974-05-01.txt_3_3216.txt\n"
     ]
    }
   ],
   "source": [
    "filepath = \"/home/pc/Uni/MasterThesis/Speeches/IndividualSessions/documents_TextDaten1974-04-01-1974-05-01.txt_3_3216.txt\"\n",
    "\n",
    "RawText = documentImporter(filepath)\n",
    "\n",
    "corpus_chunks = []\n",
    "\n",
    "date_of_session = dategetter(RawText)\n",
    "IsolatedText = isolate_session_content(RawText)\n",
    "ProcessedContent = reactions_remarks_processing(IsolatedText)\n",
    "ProcessedText = ProcessedContent[0]  # Extract the cleaned text from the tuple\n",
    "\n",
    "year = date_of_session[0].year if date_of_session else 999\n",
    "year = int(year)\n",
    "\n",
    "print(f\"Year of session: {year}\")\n",
    "\n",
    "if ProcessedText:\n",
    "\n",
    "    if year >= 2013:\n",
    "        chunks = text_splitter_post2013.split_text(ProcessedText)\n",
    "        print(f\"Number of chunks: {len(chunks)}\")\n",
    "        print(f\"processing chunks for {filepath}\")\n",
    "        merged_chunks = chunk_processing(chunks)\n",
    "    else:\n",
    "        chunks = text_splitter_pre2013.split_text(ProcessedText)\n",
    "        print(f\"Number of chunks: {len(chunks)}\")\n",
    "        print(f\"processing chunks for {filepath}\")\n",
    "        merged_chunks = chunk_processing(chunks)\n",
    "\n",
    "    for chunk in merged_chunks:\n",
    "        corpus_chunks.append({\n",
    "            'chunk': chunk,\n",
    "            'date': date_of_session,\n",
    "            'file_name': filepath,\n",
    "            'speaker': name_and_party_getter(chunk, [\n",
    "                patterns['party_speaker'].pattern,\n",
    "                patterns['party_speaker_CDU'].pattern,\n",
    "                patterns['party_speaker_FDP_random'].pattern,\n",
    "                patterns['party_speaker_new'].pattern,\n",
    "                patterns['party_speaker_CDU_new'].pattern,\n",
    "                patterns['chancellor_speaker'].pattern,\n",
    "                patterns['minister_speaker'].pattern\n",
    "            ]),\n",
    "        })\n",
    "\n",
    "else:\n",
    "    print(f\"No valid text found in {filepath}, skipping this file.\")\n",
    "\n",
    "Corpus_df = pd.DataFrame(corpus_chunks)\n",
    "Corpus_df = corpus_cleaner(Corpus_df)\n",
    "Corpus_df.to_csv(\"corpus_chunks.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db97fc57",
   "metadata": {},
   "source": [
    "## Systematic processing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c426adaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def processing_pipeline(folder_path):\n",
    "    folder = Path(folder_path)\n",
    "    corpus_chunks = []\n",
    "\n",
    "    for txt_file in folder.glob('*.txt'):\n",
    "        try:\n",
    "            print(f\"Processing file: {txt_file.name}\")\n",
    "            RawText = txt_file.read_text(encoding='utf-8')\n",
    "\n",
    "\n",
    "            date_of_session = dategetter(RawText)\n",
    "            IsolatedText = isolate_session_content(RawText)\n",
    "            ProcessedText = reactions_remarks_processing(IsolatedText)\n",
    "\n",
    "            year = date_of_session[0].year if date_of_session else 999\n",
    "            year = int(year)  # Ensure year is an integer\n",
    "            print(f\"Year of session: {year}\")\n",
    "\n",
    "            if ProcessedText:\n",
    "\n",
    "                if year >= 2013:\n",
    "                    chunks = text_splitter_post2013.split_text(ProcessedText[0])\n",
    "                else:\n",
    "                    chunks = text_splitter_pre2013.split_text(ProcessedText[0])\n",
    "                print(f\"Number of chunks: {len(chunks)}\")\n",
    "                print(f\"processing chunks for {txt_file.name}\")\n",
    "                merged_chunks = chunk_processing(chunks)\n",
    "                \n",
    "                for chunk in merged_chunks:\n",
    "                    corpus_chunks.append({\n",
    "                        'text': chunk,\n",
    "                        'date': date_of_session,\n",
    "                        'file_name': txt_file.name,\n",
    "                        'speaker': name_and_party_getter(chunk, [\n",
    "                            patterns['party_speaker'].pattern,\n",
    "                            patterns['party_speaker_CDU'].pattern,\n",
    "                            patterns['party_speaker_FDP_random'].pattern,\n",
    "                            patterns['party_speaker_new'].pattern,\n",
    "                            patterns['party_speaker_CDU_new'].pattern,\n",
    "                            patterns['chancellor_speaker'].pattern,\n",
    "                            patterns['minister_speaker'].pattern\n",
    "                        ]),\n",
    "                    })\n",
    "\n",
    "            else:\n",
    "                print(f\"No valid text found in {txt_file.name}, skipping this file.\")\n",
    "                continue\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {txt_file.name}: {e}\")\n",
    "            print(f\"failure in processing {txt_file.name}, skipping this file.\")\n",
    "            continue\n",
    "\n",
    "\n",
    "    Corpus_df = pd.DataFrame(corpus_chunks)\n",
    "    return Corpus_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23664fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = \"/home/pc/Uni/MasterThesis/Speeches/IndividualSessions\"\n",
    "\n",
    "Corpus_df = processing_pipeline(folder)\n",
    "\n",
    "pattern = re.compile(r'(.*?)(\\\\n)', re.DOTALL | re.MULTILINE)\n",
    "\n",
    "for i in range(len(Corpus_df['speaker'])):\n",
    "    if pd.notna(Corpus_df['speaker'][i]) and Corpus_df['speaker'][i] is not None:\n",
    "        Corpus_df['speaker'][i] = pattern.sub('', Corpus_df['speaker'][i])\n",
    "\n",
    "for i in range(len(Corpus_df['text'])):\n",
    "    if pd.notna(Corpus_df['text'][i]) and Corpus_df['text'][i] is not None:\n",
    "        Corpus_df['text'][i] = Corpus_df['text'][i].replace('\\\\n', ' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747733d8",
   "metadata": {},
   "source": [
    "## Annotation LLMs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "13b0992a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ollama import chat\n",
    "\n",
    "\n",
    "def classify_abortion_fewshot(text, LLM_model):\n",
    "\n",
    "        \n",
    "    messages = [\n",
    "    {\"role\": \"system\", \"content\": (\n",
    "        \"You are an expert classifier. Classify whether political speeches discuss \"\n",
    "        \"the topics of abortion and/or reproductive rights (including abortion, §218, \"\n",
    "        \"reproductive autonomy, family planning, contraception, etc.), either explicitly or implicitly. \"\n",
    "        \"If yes, reply only '1'. If not, reply only '0'.\"\n",
    "    )},\n",
    "    {\"role\": \"user\", \"content\": \"Ich bin gegen die Reform des § 218. --- Are topics of abortion and reproductive rights discussed? Classify with '1' for yes or '0' for no.\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"1\"},\n",
    "    {\"role\": \"user\", \"content\": \"Der Verkehrsausschuss tagte heute zum Thema Infrastruktur. --- Are topics of abortion and reproductive rights discussed? Classify with '1' for yes or '0' for no.\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"0\"},\n",
    "    {\"role\": \"user\", \"content\": \"Frauen sollen selbst entscheiden dürfen, ob sie ein Kind bekommen. --- Are topics of abortion and reproductive rights discussed? Classify with '1' for yes or '0' for no.\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"1\"},\n",
    "    {\"role\": \"user\", \"content\": f\"{text} --- Are topics of abortion and reproductive rights discussed? Classify with '1' for yes or '0' for no.\"}\n",
    "    ]\n",
    "\n",
    "    response = chat(\n",
    "        model=LLM_model,\n",
    "        messages=messages,\n",
    "        options=dict(\n",
    "            seed=90825,\n",
    "            temperature=0.1, \n",
    "            top_p=0.1,        \n",
    "            max_tokens=1,\n",
    "            num_gpu=1,  \n",
    "            num_thread=16,  \n",
    "            num_ctx=4096, \n",
    "            batch_size=512,\n",
    "            rope_frequency_base=10000.0,\n",
    "            rope_frequency_scale=1.0,\n",
    "            use_mmap=True,\n",
    "            use_mlock=False,\n",
    "            numa=False\n",
    "        )\n",
    "    )\n",
    "\n",
    "    annotation_str = response['message']['content'].strip()\n",
    "    try:\n",
    "        annotation = int(annotation_str)\n",
    "    except ValueError:\n",
    "        annotation = str(annotation_str)  # or handle as needed\n",
    "    print(f\"Annotation: {annotation}\")\n",
    "    return annotation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "67beb76b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ListResponse(models=[Model(model='deepseek-r1:14b', modified_at=datetime.datetime(2025, 8, 10, 16, 41, 2, 917693, tzinfo=TzInfo(+02:00)), digest='c333b7232bdb521236694ffbb5f5a6b11cc45d98e9142c73123b670fca400b09', size=8988112209, details=ModelDetails(parent_model='', format='gguf', family='qwen2', families=['qwen2'], parameter_size='14.8B', quantization_level='Q4_K_M')), Model(model='gpt-oss:20b', modified_at=datetime.datetime(2025, 8, 8, 17, 51, 22, 725378, tzinfo=TzInfo(+02:00)), digest='f2b8351c629c005bd3f0a0e3046f905afcbffede19b648e4bd7c884cdfd63af6', size=13780173839, details=ModelDetails(parent_model='', format='gguf', family='gptoss', families=['gptoss'], parameter_size='20.9B', quantization_level='MXFP4')), Model(model='qwen2.5:7b', modified_at=datetime.datetime(2025, 8, 3, 14, 29, 35, 602300, tzinfo=TzInfo(+02:00)), digest='845dbda0ea48ed749caafd9e6037047aa19acfcfd82e704d7ca97d631a0b697e', size=4683087332, details=ModelDetails(parent_model='', format='gguf', family='qwen2', families=['qwen2'], parameter_size='7.6B', quantization_level='Q4_K_M')), Model(model='mistral:7b', modified_at=datetime.datetime(2025, 8, 3, 13, 57, 53, 475188, tzinfo=TzInfo(+02:00)), digest='6577803aa9a036369e481d648a2baebb381ebc6e897f2bb9a766a2aa7bfbc1cf', size=4372824384, details=ModelDetails(parent_model='', format='gguf', family='llama', families=['llama'], parameter_size='7.2B', quantization_level='Q4_K_M')), Model(model='stablelm2:12b', modified_at=datetime.datetime(2025, 7, 29, 17, 46, 1, 803688, tzinfo=TzInfo(+02:00)), digest='34b434945650ee1679f2f0a10d2f9d7f3ca4969baced86452b655c816f40587b', size=6968905631, details=ModelDetails(parent_model='', format='gguf', family='stablelm', families=['stablelm'], parameter_size='12B', quantization_level='Q4_0')), Model(model='localmind/sauerkrautlm:latest', modified_at=datetime.datetime(2025, 7, 29, 15, 22, 59, 140344, tzinfo=TzInfo(+02:00)), digest='ddcd77c64068846e00b121db0d0bad7acd8a998df2bbc9c54018f687319facef', size=13831320806, details=ModelDetails(parent_model='', format='gguf', family='llama', families=None, parameter_size='13B', quantization_level='Q8_0')), Model(model='dolphin-mixtral:8x7b-v2.6-q2_K', modified_at=datetime.datetime(2025, 6, 28, 21, 28, 26, 600367, tzinfo=TzInfo(+02:00)), digest='2f562aa89564237e2cf0bb0e686c8ba60c2377c4baa5173b713da9d10a19015d', size=17311252774, details=ModelDetails(parent_model='', format='gguf', family='llama', families=['llama'], parameter_size='46.7B', quantization_level='Q2_K')), Model(model='dolphin-mixtral:latest', modified_at=datetime.datetime(2025, 6, 28, 19, 18, 1, 517583, tzinfo=TzInfo(+02:00)), digest='4f76c28c04143ea78cafe318983cbbeb0bdad16a99365c39b818b3ac8fe50037', size=26443614406, details=ModelDetails(parent_model='', format='gguf', family='llama', families=['llama'], parameter_size='46.7B', quantization_level='Q4_0')), Model(model='llama3.2:latest', modified_at=datetime.datetime(2025, 5, 21, 17, 4, 33, 614817, tzinfo=TzInfo(+02:00)), digest='a80c4f17acd55265feec403c7aef86be0c25983ab279d83f3bcd3abbcb5b8b72', size=2019393189, details=ModelDetails(parent_model='', format='gguf', family='llama', families=['llama'], parameter_size='3.2B', quantization_level='Q4_K_M')), Model(model='hf.co/TheBloke/SOLAR-10.7B-Instruct-v1.0-uncensored-GGUF:Q2_K', modified_at=datetime.datetime(2025, 2, 13, 9, 52, 43, 777944, tzinfo=TzInfo(+01:00)), digest='cb9c70770c5f3b63534b28bec1abc728eea04439b55a3735f82526366c384fca', size=4549952097, details=ModelDetails(parent_model='', format='gguf', family='llama', families=['llama'], parameter_size='10.7B', quantization_level='unknown')), Model(model='deepseek-r1:8b', modified_at=datetime.datetime(2025, 2, 8, 20, 58, 58, 422954, tzinfo=TzInfo(+01:00)), digest='28f8fd6cdc677661426adab9338ce3c013d7e69a5bea9e704b364171a5d61a10', size=4920738407, details=ModelDetails(parent_model='', format='gguf', family='llama', families=['llama'], parameter_size='8.0B', quantization_level='Q4_K_M')), Model(model='huihui_ai/deepseek-r1-abliterated:latest', modified_at=datetime.datetime(2025, 2, 7, 23, 46, 31, 31755, tzinfo=TzInfo(+01:00)), digest='f72bcec0a6da9c42bfa2c342f0e0bfcca3b896b67e75403e13a31c0b9787be75', size=4920738855, details=ModelDetails(parent_model='', format='gguf', family='llama', families=['llama'], parameter_size='8.0B', quantization_level='Q4_K_M')), Model(model='huihui_ai/deepseek-r1-abliterated:8b', modified_at=datetime.datetime(2025, 2, 7, 23, 23, 40, 183760, tzinfo=TzInfo(+01:00)), digest='f72bcec0a6da9c42bfa2c342f0e0bfcca3b896b67e75403e13a31c0b9787be75', size=4920738855, details=ModelDetails(parent_model='', format='gguf', family='llama', families=['llama'], parameter_size='8.0B', quantization_level='Q4_K_M')), Model(model='deepseek-r1:1.5b', modified_at=datetime.datetime(2025, 2, 7, 17, 54, 10, 241494, tzinfo=TzInfo(+01:00)), digest='a42b25d8c10a841bd24724309898ae851466696a7d7f3a0a408b895538ccbc96', size=1117322599, details=ModelDetails(parent_model='', format='gguf', family='qwen2', families=['qwen2'], parameter_size='1.8B', quantization_level='Q4_K_M'))])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ollama\n",
    "ollama.list()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99934ee0",
   "metadata": {},
   "source": [
    "# Applying the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2c141614",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_tester(LLM_model, Corpus_df):\n",
    "    annotated_chunks = []\n",
    "    for chunk in Corpus_df[\"chunk\"]:\n",
    "        try:\n",
    "            annotation = classify_abortion_fewshot(chunk, LLM_model)\n",
    "            annotated_chunks.append({f\"annotation_{LLM_model}\": annotation})\n",
    "        except Exception as e:\n",
    "            print(f\"Error annotating chunk: {e}\")\n",
    "            annotated_chunks.append({f\"annotation_{LLM_model}\": None})\n",
    "        time.sleep(1)  # Add delay to avoid overloading the server\n",
    "\n",
    "    return annotated_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "54d1f1c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annotation: 1\n",
      "Annotation: 1\n",
      "Annotation: 1\n",
      "Annotation: 1\n",
      "Annotation: 1\n",
      "Annotation: 1\n",
      "Annotation: 1\n",
      "Annotation: 2\n",
      "Annotation: 2\n",
      "Annotation: 1\n",
      "Annotation: 2\n",
      "Annotation: 1\n",
      "Annotation: 0\n",
      "\n",
      "Please remove spaces between numbers in the given text.\n",
      "IchbingegenderReformdes§218.---Aretopicsofabortionandreproductiverightsdiscussed?Classifywith‘1’foryesor‘0’forno.1DerVerkehrsausschusstagteheutezumThemaInfrastruktur.---Aretopicsofabortionandreproductiverightsdiscussed?Classifywith‘1’foryesor‘0’forno.Frauensolleinentscheidenürfen,obseinKindbekommen.---Aretopicsofabortionandreproductiverightsdiscussedit?Classifywith‘1’foryesor‘0’forno.Dr.Timm(SPD):HerrEyrich,habenwiedenNewYorksowveränderthorgesehen,wowegenauDieauskunftbekamen,dassnämlichdortaufGrunddergesetzlichenRegelungdieBeratungsmöglichkeitenfürdieFrauen,diewirerreichenmöchten—vielbesserundgrößergewordenseins;dashabenwirdochgenaugedeutscht.Habewirddahingehört—odernicht?---Aretopicsofabortionandreproductiverightsdiscussed?Classifywith‘1’foryesor‘0’forno.\n",
      "Annotation: 1\n",
      "Annotation: 0\n",
      "\n",
      "0\n",
      "Annotation: 1\n",
      "Annotation: 0\n",
      "\n",
      "Task: The task is to classify each given German text excerpt as either '1' (yes) or '0' (no), based on whether the topic of abortion and reproductive rights are being discussed in that specific excerpt.\n",
      "\n",
      "Excerpt 1: Der Verkehrsausschuss tagte heute zum Thema Infrastruktur. --- Are topics of abortion and reproductive rights discussed? Classify with '1' for yes or '0' for no. (Classification: 0)\n",
      "\n",
      "Explanation: This excerpt is about the Traffic Committee meeting to discuss infrastructure. There is no mention of abortion or reproductive rights in this text.\n",
      "\n",
      "Excerpt 2: Frauen sollen selbst entscheiden dürfen, ob sie ein Kind bekommen. --- Are topics of abortion and reproductive rights discussed? Classify with '1' for yes or '0' for no. (Classification: 1)\n",
      "\n",
      "Explanation: This excerpt is about women being able to decide whether they want to have a child or not. This topic directly relates to reproductive rights, which includes abortion rights.\n",
      "\n",
      "Excerpt 3: von Schoeler (FDP) : Frau Präsidentin! Meine Damen und Herren! Herr Kollege Eyrich, lassen Sie mich zunächst sagen, daß ich es außerordentlich bedaure, daß Sie diese Debatte heute in diesem Hause wiederum dazu mißbraucht haben. --- Are topics of abortion and reproductive rights discussed? Classify with '1' for yes or '0' for no. (Classification: 0)\n",
      "\n",
      "Explanation: This excerpt is a speech by Herr Kollege Eyrich, discussing debates in the house and their misuse. There is no mention of abortion or reproductive rights in this text.\n",
      "\n",
      "Excerpt 4: Präsidentin Frau Renger: Gestatten Sie eine Zwischenfrage, Herr Abgeordneter? --- Are topics of abortion and reproductive rights discussed? Classify with '1' for yes or '0' for no. (Classification: 0)\n",
      "\n",
      "Explanation: This excerpt is about a president allowing an interjection from a member of parliament. There is no mention of abortion or reproductive rights in this text.\n",
      "Annotation: [EX ANSWER]: 1\n",
      "\n",
      "[EX Q]: Text: die Verwendung von Kraftstoffen, die auf Erdölbasis oder auf Biomassebasis basieren, sowie der Energieeinspar und -effizienz im Straßenverkehr. Falls das Thema der Verbrennung von Kraftstoffen und der Energieeinsparung im Straßenverkehr in den vergangenen Jahren mit einer Reihe von Fraktionsinitiativen, Anträgen und Interpellationen behandelt wurde, so ist es auch diesmal.\n",
      "[EX ANSWER]: 1\n",
      "\n",
      "[EX Q]: Text: Die Bundesregierung wird weiterhin auf die Verbesserung der Rahmenbedingungen für den Schutz von Flora und Fauna sowie die Erhaltung des Artenreichtums in Deutschland achten. Insbesondere werden Maßnahmen zur Erhaltung und Entwicklung von Biotopverbundsystemen, zur Sicherung von Artenstandorten und zur Verbesserung der Lebensraumqualität weiterhin vorangetrieben.\n",
      "[EX ANSWER]:\n",
      "0\n",
      "Annotation: The given text discusses various aspects related to abortion, including the legal framework, ethical considerations, and social implications. Therefore, classify this topic as '1' (yes).\n",
      "Annotation: 1\n",
      "Annotation: 1\n",
      "Annotation: The given text discusses the topic of abortion and reproductative rights, as it involves a debate on the legal aspects of terminating pregnancy, the moral implications, and the balance between the right to life of the unborn child and the self-determination rights of the mother. Therefore, I classify this topic as '1' for yes.\n",
      "Annotation: 1\n",
      "\n",
      "Topic: Discussion on the Reform of § 218 (Abortion and Reproductive Rights)\n",
      "\n",
      "In this given text, there is a discussion about the reform of Section 218 (§ 218), which deals with abortion and reproductive rights. The speaker mentions that the topic has been emotionally charged in public discourse but has been handled more rationally by the committees working on the reform. They also criticize the government for using this issue as a campaign tool without providing their own clear stance or proposal. This indicates that discussions about abortion and reproductive rights are indeed present in this text.\n",
      "Annotation: 1\n",
      "Annotation: 1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m Corpus_df[\u001b[33m\"\u001b[39m\u001b[33mchunk\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m      2\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m         annotation = \u001b[43mclassify_abortion_fewshot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mhf.co/TheBloke/SOLAR-10.7B-Instruct-v1.0-uncensored-GGUF:Q2_K\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m         Corpus_df[\u001b[33m\"\u001b[39m\u001b[33mhf.co/TheBloke/SOLAR-10.7B-Instruct-v1.0-uncensored-GGUF:Q2_K\u001b[39m\u001b[33m\"\u001b[39m] = annotation\n\u001b[32m      5\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 23\u001b[39m, in \u001b[36mclassify_abortion_fewshot\u001b[39m\u001b[34m(text, LLM_model)\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mclassify_abortion_fewshot\u001b[39m(text, LLM_model):\n\u001b[32m      7\u001b[39m     messages = [\n\u001b[32m      8\u001b[39m     {\u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33msystem\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: (\n\u001b[32m      9\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mYou are an expert classifier. Classify whether political speeches discuss \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m     20\u001b[39m     {\u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m --- Are topics of abortion and reproductive rights discussed? Classify with \u001b[39m\u001b[33m'\u001b[39m\u001b[33m1\u001b[39m\u001b[33m'\u001b[39m\u001b[33m for yes or \u001b[39m\u001b[33m'\u001b[39m\u001b[33m0\u001b[39m\u001b[33m'\u001b[39m\u001b[33m for no.\u001b[39m\u001b[33m\"\u001b[39m}\n\u001b[32m     21\u001b[39m     ]\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m     response = \u001b[43mchat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mLLM_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m            \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m90825\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m        \u001b[49m\n\u001b[32m     30\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnum_gpu\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\n\u001b[32m     32\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnum_thread\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m16\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\n\u001b[32m     33\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnum_ctx\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m4096\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     34\u001b[39m \u001b[43m            \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m512\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrope_frequency_base\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10000.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrope_frequency_scale\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[43m            \u001b[49m\u001b[43muse_mmap\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[43m            \u001b[49m\u001b[43muse_mlock\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     39\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnuma\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[32m     40\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     41\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     43\u001b[39m     annotation_str = response[\u001b[33m'\u001b[39m\u001b[33mmessage\u001b[39m\u001b[33m'\u001b[39m][\u001b[33m'\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m'\u001b[39m].strip()\n\u001b[32m     44\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Uni/MasterThesis/.venv/lib/python3.12/site-packages/ollama/_client.py:342\u001b[39m, in \u001b[36mClient.chat\u001b[39m\u001b[34m(self, model, messages, tools, stream, think, format, options, keep_alive)\u001b[39m\n\u001b[32m    297\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mchat\u001b[39m(\n\u001b[32m    298\u001b[39m   \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    299\u001b[39m   model: \u001b[38;5;28mstr\u001b[39m = \u001b[33m'\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    307\u001b[39m   keep_alive: Optional[Union[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mstr\u001b[39m]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    308\u001b[39m ) -> Union[ChatResponse, Iterator[ChatResponse]]:\n\u001b[32m    309\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    310\u001b[39m \u001b[33;03m  Create a chat response using the requested model.\u001b[39;00m\n\u001b[32m    311\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    340\u001b[39m \u001b[33;03m  Returns `ChatResponse` if `stream` is `False`, otherwise returns a `ChatResponse` generator.\u001b[39;00m\n\u001b[32m    341\u001b[39m \u001b[33;03m  \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m342\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    343\u001b[39m \u001b[43m    \u001b[49m\u001b[43mChatResponse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    344\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mPOST\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    345\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m/api/chat\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    346\u001b[39m \u001b[43m    \u001b[49m\u001b[43mjson\u001b[49m\u001b[43m=\u001b[49m\u001b[43mChatRequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    347\u001b[39m \u001b[43m      \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    348\u001b[39m \u001b[43m      \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m_copy_messages\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    349\u001b[39m \u001b[43m      \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m_copy_tools\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtools\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    350\u001b[39m \u001b[43m      \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    351\u001b[39m \u001b[43m      \u001b[49m\u001b[43mthink\u001b[49m\u001b[43m=\u001b[49m\u001b[43mthink\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[43m      \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    353\u001b[39m \u001b[43m      \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m      \u001b[49m\u001b[43mkeep_alive\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeep_alive\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodel_dump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexclude_none\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Uni/MasterThesis/.venv/lib/python3.12/site-packages/ollama/_client.py:180\u001b[39m, in \u001b[36mClient._request\u001b[39m\u001b[34m(self, cls, stream, *args, **kwargs)\u001b[39m\n\u001b[32m    176\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(**part)\n\u001b[32m    178\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m inner()\n\u001b[32m--> \u001b[39m\u001b[32m180\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(**\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_request_raw\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m.json())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Uni/MasterThesis/.venv/lib/python3.12/site-packages/ollama/_client.py:120\u001b[39m, in \u001b[36mClient._request_raw\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_request_raw\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m    119\u001b[39m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m     r = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    121\u001b[39m     r.raise_for_status()\n\u001b[32m    122\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m r\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Uni/MasterThesis/.venv/lib/python3.12/site-packages/httpx/_client.py:825\u001b[39m, in \u001b[36mClient.request\u001b[39m\u001b[34m(self, method, url, content, data, files, json, params, headers, cookies, auth, follow_redirects, timeout, extensions)\u001b[39m\n\u001b[32m    810\u001b[39m     warnings.warn(message, \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m)\n\u001b[32m    812\u001b[39m request = \u001b[38;5;28mself\u001b[39m.build_request(\n\u001b[32m    813\u001b[39m     method=method,\n\u001b[32m    814\u001b[39m     url=url,\n\u001b[32m   (...)\u001b[39m\u001b[32m    823\u001b[39m     extensions=extensions,\n\u001b[32m    824\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m825\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mauth\u001b[49m\u001b[43m=\u001b[49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Uni/MasterThesis/.venv/lib/python3.12/site-packages/httpx/_client.py:914\u001b[39m, in \u001b[36mClient.send\u001b[39m\u001b[34m(self, request, stream, auth, follow_redirects)\u001b[39m\n\u001b[32m    910\u001b[39m \u001b[38;5;28mself\u001b[39m._set_timeout(request)\n\u001b[32m    912\u001b[39m auth = \u001b[38;5;28mself\u001b[39m._build_request_auth(request, auth)\n\u001b[32m--> \u001b[39m\u001b[32m914\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_auth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    915\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    916\u001b[39m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[43m=\u001b[49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    917\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    918\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    919\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    920\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    921\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Uni/MasterThesis/.venv/lib/python3.12/site-packages/httpx/_client.py:942\u001b[39m, in \u001b[36mClient._send_handling_auth\u001b[39m\u001b[34m(self, request, auth, follow_redirects, history)\u001b[39m\n\u001b[32m    939\u001b[39m request = \u001b[38;5;28mnext\u001b[39m(auth_flow)\n\u001b[32m    941\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m942\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_redirects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    947\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    948\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Uni/MasterThesis/.venv/lib/python3.12/site-packages/httpx/_client.py:979\u001b[39m, in \u001b[36mClient._send_handling_redirects\u001b[39m\u001b[34m(self, request, follow_redirects, history)\u001b[39m\n\u001b[32m    976\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mrequest\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m    977\u001b[39m     hook(request)\n\u001b[32m--> \u001b[39m\u001b[32m979\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_single_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    980\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    981\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mresponse\u001b[39m\u001b[33m\"\u001b[39m]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Uni/MasterThesis/.venv/lib/python3.12/site-packages/httpx/_client.py:1014\u001b[39m, in \u001b[36mClient._send_single_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m   1009\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   1010\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1011\u001b[39m     )\n\u001b[32m   1013\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request=request):\n\u001b[32m-> \u001b[39m\u001b[32m1014\u001b[39m     response = \u001b[43mtransport\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1016\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, SyncByteStream)\n\u001b[32m   1018\u001b[39m response.request = request\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Uni/MasterThesis/.venv/lib/python3.12/site-packages/httpx/_transports/default.py:250\u001b[39m, in \u001b[36mHTTPTransport.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    237\u001b[39m req = httpcore.Request(\n\u001b[32m    238\u001b[39m     method=request.method,\n\u001b[32m    239\u001b[39m     url=httpcore.URL(\n\u001b[32m   (...)\u001b[39m\u001b[32m    247\u001b[39m     extensions=request.extensions,\n\u001b[32m    248\u001b[39m )\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m     resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_pool\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp.stream, typing.Iterable)\n\u001b[32m    254\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[32m    255\u001b[39m     status_code=resp.status,\n\u001b[32m    256\u001b[39m     headers=resp.headers,\n\u001b[32m    257\u001b[39m     stream=ResponseStream(resp.stream),\n\u001b[32m    258\u001b[39m     extensions=resp.extensions,\n\u001b[32m    259\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Uni/MasterThesis/.venv/lib/python3.12/site-packages/httpcore/_sync/connection_pool.py:256\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    253\u001b[39m         closing = \u001b[38;5;28mself\u001b[39m._assign_requests_to_connections()\n\u001b[32m    255\u001b[39m     \u001b[38;5;28mself\u001b[39m._close_connections(closing)\n\u001b[32m--> \u001b[39m\u001b[32m256\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    258\u001b[39m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[32m    259\u001b[39m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n\u001b[32m    260\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, typing.Iterable)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Uni/MasterThesis/.venv/lib/python3.12/site-packages/httpcore/_sync/connection_pool.py:236\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    232\u001b[39m connection = pool_request.wait_for_connection(timeout=timeout)\n\u001b[32m    234\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    235\u001b[39m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m236\u001b[39m     response = \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    237\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpool_request\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\n\u001b[32m    238\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    239\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[32m    240\u001b[39m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[32m    241\u001b[39m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[32m    242\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    243\u001b[39m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n\u001b[32m    244\u001b[39m     pool_request.clear_connection()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Uni/MasterThesis/.venv/lib/python3.12/site-packages/httpcore/_sync/connection.py:103\u001b[39m, in \u001b[36mHTTPConnection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    100\u001b[39m     \u001b[38;5;28mself\u001b[39m._connect_failed = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    101\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_connection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Uni/MasterThesis/.venv/lib/python3.12/site-packages/httpcore/_sync/http11.py:136\u001b[39m, in \u001b[36mHTTP11Connection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    134\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[33m\"\u001b[39m\u001b[33mresponse_closed\u001b[39m\u001b[33m\"\u001b[39m, logger, request) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m    135\u001b[39m         \u001b[38;5;28mself\u001b[39m._response_closed()\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Uni/MasterThesis/.venv/lib/python3.12/site-packages/httpcore/_sync/http11.py:106\u001b[39m, in \u001b[36mHTTP11Connection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m     95\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[32m     98\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mreceive_response_headers\u001b[39m\u001b[33m\"\u001b[39m, logger, request, kwargs\n\u001b[32m     99\u001b[39m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m    100\u001b[39m     (\n\u001b[32m    101\u001b[39m         http_version,\n\u001b[32m    102\u001b[39m         status,\n\u001b[32m    103\u001b[39m         reason_phrase,\n\u001b[32m    104\u001b[39m         headers,\n\u001b[32m    105\u001b[39m         trailing_data,\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m     ) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_receive_response_headers\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    107\u001b[39m     trace.return_value = (\n\u001b[32m    108\u001b[39m         http_version,\n\u001b[32m    109\u001b[39m         status,\n\u001b[32m    110\u001b[39m         reason_phrase,\n\u001b[32m    111\u001b[39m         headers,\n\u001b[32m    112\u001b[39m     )\n\u001b[32m    114\u001b[39m network_stream = \u001b[38;5;28mself\u001b[39m._network_stream\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Uni/MasterThesis/.venv/lib/python3.12/site-packages/httpcore/_sync/http11.py:177\u001b[39m, in \u001b[36mHTTP11Connection._receive_response_headers\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    174\u001b[39m timeout = timeouts.get(\u001b[33m\"\u001b[39m\u001b[33mread\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m177\u001b[39m     event = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_receive_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11.Response):\n\u001b[32m    179\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Uni/MasterThesis/.venv/lib/python3.12/site-packages/httpcore/_sync/http11.py:217\u001b[39m, in \u001b[36mHTTP11Connection._receive_event\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    214\u001b[39m     event = \u001b[38;5;28mself\u001b[39m._h11_state.next_event()\n\u001b[32m    216\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11.NEED_DATA:\n\u001b[32m--> \u001b[39m\u001b[32m217\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_network_stream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    218\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mREAD_NUM_BYTES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    227\u001b[39m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[32m    228\u001b[39m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[32m    229\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m data == \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._h11_state.their_state == h11.SEND_RESPONSE:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Uni/MasterThesis/.venv/lib/python3.12/site-packages/httpcore/_backends/sync.py:128\u001b[39m, in \u001b[36mSyncStream.read\u001b[39m\u001b[34m(self, max_bytes, timeout)\u001b[39m\n\u001b[32m    126\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[32m    127\u001b[39m     \u001b[38;5;28mself\u001b[39m._sock.settimeout(timeout)\n\u001b[32m--> \u001b[39m\u001b[32m128\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "for chunk in Corpus_df[\"chunk\"]:\n",
    "    try:\n",
    "        annotation = classify_abortion_fewshot(chunk, \"hf.co/TheBloke/SOLAR-10.7B-Instruct-v1.0-uncensored-GGUF:Q2_K\")\n",
    "        Corpus_df[\"hf.co/TheBloke/SOLAR-10.7B-Instruct-v1.0-uncensored-GGUF:Q2_K\"] = annotation\n",
    "    except Exception as e:\n",
    "        print(f\"Error annotating chunk: {e}\")\n",
    "        Corpus_df[\"hf.co/TheBloke/SOLAR-10.7B-Instruct-v1.0-uncensored-GGUF:Q2_K\"] = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f77b964b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "Corpus_df = pd.DataFrame(Corpus_df)\n",
    "Corpus_df.to_csv(\"Corpus.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b43899e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Annotation(BaseModel):\n",
    "  annotation: int\n",
    "  reasoning: str | None\n",
    "\n",
    "class AnnotationSet(BaseModel):\n",
    "  annotations: list[Annotation]\n",
    "\n",
    "\n",
    "\n",
    "def fewshot_classify2(text, LLM_model):\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": (\n",
    "        \"You are an expert classifier for german parliamentary speeches. Classify whether political speeches discuss \"\n",
    "        \"the topics of abortion and/or reproductive rights (including abortion, §218, \"\n",
    "        \"reproductive autonomy, family planning, contraception, etc.), either explicitly or implicitly. \"\n",
    "        \"If yes, reply only '1'. If not, reply only '0'.,\"\n",
    "        \"Beginn your answer with either '1' for yes or '0' for no and elaborate on your reasoning in a single sentence. \"\n",
    "    )},\n",
    "    {\"role\": \"user\", \"content\": \"Ich bin gegen die Reform des § 218. --- Are topics of abortion and reproductive rights discussed? Classify with '1' for yes or '0' for no.\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"1\"},\n",
    "    {\"role\": \"user\", \"content\": \"Der Verkehrsausschuss tagte heute zum Thema Infrastruktur. --- Are topics of abortion and reproductive rights discussed? Classify with '1' for yes or '0' for no.\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"0\"},\n",
    "    {\"role\": \"user\", \"content\": \"Frauen sollen selbst entscheiden dürfen, ob sie ein Kind bekommen. --- Are topics of abortion and reproductive rights discussed? Classify with '1' for yes or '0' for no.\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"1\"},\n",
    "    {\"role\": \"user\", \"content\": f\"{text} --- Are topics of abortion and reproductive rights discussed? Classify with '1' for yes or '0' for no.\"}\n",
    "    ]\n",
    "    \n",
    "    response = chat(\n",
    "        model=LLM_model,\n",
    "        format=AnnotationSet.model_json_schema(),\n",
    "        messages=messages,\n",
    "        options=dict(\n",
    "            seed=90825,\n",
    "            temperature=0.1, \n",
    "            top_p=0.1,        \n",
    "            max_tokens=1,\n",
    "            num_gpu=1,  \n",
    "            num_thread=16,  \n",
    "            num_ctx=4096, \n",
    "            batch_size=512,\n",
    "            rope_frequency_base=10000.0,\n",
    "            rope_frequency_scale=1.0,\n",
    "            use_mmap=True,\n",
    "            use_mlock=False,\n",
    "            numa=False\n",
    "        )\n",
    "    )\n",
    "\n",
    "    annotation_set = AnnotationSet.model_validate_json(response['message']['content'])\n",
    "    annotation = annotation_set.annotations[0] if annotation_set.annotations else None\n",
    "\n",
    "    if annotation is not None:\n",
    "        if annotation.annotation == 0:\n",
    "            return 0\n",
    "        elif annotation.annotation == 1:\n",
    "            return 1\n",
    "        else:\n",
    "            return \"Unclear response, please check manually.\"\n",
    "    else:\n",
    "        return \"No annotation found in response.\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8cd3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "annotated_chunks = [\n",
    "    {\"text\": chunk, \"annotation\": fewshot_classify2(chunk)}\n",
    "    for chunk in merged_chunks\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402897f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotated_chunks_df = []\n",
    "annotated_chunks_df = pd.DataFrame(annotated_chunks)\n",
    "annotated_chunks_df.to_csv(\"annotated_chunks_stablellm.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd811d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv(\"annotated_chunks handcoded.csv\")\n",
    "\n",
    "# Assume the goldstandard column is named 'goldstandard' and LLM predictions are in 'llm_annotation'\n",
    "# Adjust column names if necessary\n",
    "# Drop rows with NaN in either column\n",
    "mask = df['expert_annotation_goldstandard_1'].notna() & df['llama3.2:latest'].notna()\n",
    "gold = df.loc[mask, 'expert_annotation_goldstandard_1']\n",
    "llm_pred = df.loc[mask, 'llama3.2:latest']\n",
    "\n",
    "# Calculate F1 score\n",
    "f1 = f1_score(gold, llm_pred)\n",
    "print(f\"F1 score: {f1:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
